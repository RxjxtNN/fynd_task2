{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99724e8d78d44e6fb2747f5ea1e8de0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_426306f8480b42edae64fc0f1a3c3350",
              "IPY_MODEL_8d32558ae6fa468f9fbb589e28c6eb8b",
              "IPY_MODEL_2abace3933d14889b5ec93f260e62107"
            ],
            "layout": "IPY_MODEL_503eed4aea074d8c900a7080a9e53fe2"
          }
        },
        "426306f8480b42edae64fc0f1a3c3350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4cd97fcdd6d4ba09c9b2d02285b75ab",
            "placeholder": "​",
            "style": "IPY_MODEL_ae5342e2e06d4872ad39229d816e1ab2",
            "value": "Direct-run1:  55%"
          }
        },
        "8d32558ae6fa468f9fbb589e28c6eb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_778f286eb6894d4195dd389419f9d62c",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90ba639a6543440eb379d39e2eaa0700",
            "value": 11
          }
        },
        "2abace3933d14889b5ec93f260e62107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_219b59c46bf34f589db2d52d70c575cb",
            "placeholder": "​",
            "style": "IPY_MODEL_f82ecb2b16af41b8a25803404a4bf3ed",
            "value": " 11/20 [00:36&lt;00:19,  2.13s/it]"
          }
        },
        "503eed4aea074d8c900a7080a9e53fe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4cd97fcdd6d4ba09c9b2d02285b75ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae5342e2e06d4872ad39229d816e1ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "778f286eb6894d4195dd389419f9d62c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ba639a6543440eb379d39e2eaa0700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "219b59c46bf34f589db2d52d70c575cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f82ecb2b16af41b8a25803404a4bf3ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "99724e8d78d44e6fb2747f5ea1e8de0d",
            "426306f8480b42edae64fc0f1a3c3350",
            "8d32558ae6fa468f9fbb589e28c6eb8b",
            "2abace3933d14889b5ec93f260e62107",
            "503eed4aea074d8c900a7080a9e53fe2",
            "b4cd97fcdd6d4ba09c9b2d02285b75ab",
            "ae5342e2e06d4872ad39229d816e1ab2",
            "778f286eb6894d4195dd389419f9d62c",
            "90ba639a6543440eb379d39e2eaa0700",
            "219b59c46bf34f589db2d52d70c575cb",
            "f82ecb2b16af41b8a25803404a4bf3ed"
          ]
        },
        "id": "cXE1nrxHn7OR",
        "outputId": "9ab1f304-b598-40ba-c754-43e785f3f2d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your Google Gemini API Key (input hidden): ··········\n",
            "Gemini configured for model: gemini-2.5-flash-lite\n",
            "Found local 'yelp.csv' -- using that.\n",
            "\n",
            "================================================================================\n",
            "Running experiment: Direct\n",
            "Prompt intention: Direct instruction with hard JSON schema enforcement to eliminate formatting errors.\n",
            "Prompt skeleton (example):\n",
            "\n",
            "You are a strict JSON-only API.\n",
            "\n",
            "Task:\n",
            "Given a Yelp review, predict the star rating from 1 to 5.\n",
            "\n",
            "Rules:\n",
            "- Output ONLY valid JSON.\n",
            "- Do NOT add markdown.\n",
            "- Do NOT add comments.\n",
            "- Do NOT add extra text.\n",
            "- The output MUST match this schema exactly:\n",
            "\n",
            "{\n",
            "  \"predicted_stars\": 1-5 integer,\n",
            "  \"explanation\": \"one short sentence\"\n",
            "}\n",
            "\n",
            "Review:\n",
            "\"There was hair in the food. Not just 1 but 2 in the same Bhel Puri dish.\n",
            "\n",
            "The whole experience was pretty bad. There is no indication of how you order your food and when you finally figure it out the food does not taste good. We checked sodas on the order sheet and when no sodas showed up we asked if we were supposed to go and find our own sodas from the front of the store and were told that that was the case.\n",
            "\n",
            "Also the guy at the counter is down right dismissi\n",
            "--------------------------------------------------------------------------------\n",
            "Run 1/1 for Direct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Direct-run1:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99724e8d78d44e6fb2747f5ea1e8de0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1113.18ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 708.07ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 684.33ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 632.37ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 609.57ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 809.19ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1402818540.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             response = model.generate_content(\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTooManyRequests\u001b[0m: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash-lite\nPlease retry in 11.025530252s.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1402818540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mconfigure_gemini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAMPLE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;31m# Save summary to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1402818540.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(sample_size)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                         \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                         \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                         \u001b[0mok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1402818540.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LLM call failed after retries.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# Yelp Reviews - Prompting Experiments (Colab single cell)\n",
        "# - Loads Kaggle dataset (omkarsabnis/yelp-reviews-dataset) or accepts user-uploaded CSV\n",
        "# - Samples ~200 rows\n",
        "# - Implements 3 prompting strategies (Direct, Few-Shot, Chain-of-Thought)\n",
        "# - Runs each strategy 3 times for reliability measurement\n",
        "# - Computes: Accuracy, MAE, JSON Validity Rate, Reliability / Consistency\n",
        "# - Saves results and prints a comparison table + short discussion\n",
        "# =================================================================================\n",
        "\n",
        "# ---------------------------\n",
        "# Standard library imports\n",
        "# ---------------------------\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import statistics\n",
        "import tempfile\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# ---------------------------\n",
        "# Third-party imports\n",
        "# ---------------------------\n",
        "# (Installs are quiet; Colab will keep them if already installed)\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "    from tqdm.notebook import tqdm\n",
        "except Exception:\n",
        "    !pip install -q pandas numpy matplotlib seaborn scikit-learn tqdm\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "\n",
        "# ---------------------------\n",
        "# Optional: Gemini (Google) config\n",
        "# If you want to use Google Gemini, set GEMINI_API_KEY environment or provide when prompted.\n",
        "# If you prefer another LLM, adapt the `call_model` function below.\n",
        "# ---------------------------\n",
        "USE_GEMINI = True\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "except Exception:\n",
        "    # If google generative isn't installed, install it but continue gracefully.\n",
        "    !pip install -q google-generativeai\n",
        "    import google.generativeai as genai\n",
        "\n",
        "# ---------------------------\n",
        "# Settings (tuneable)\n",
        "# ---------------------------\n",
        "SAMPLE_SIZE = 20          # recommended ~200 rows\n",
        "RUNS_PER_PROMPT = 1        # number of independent runs per prompt for reliability\n",
        "DELAY_BETWEEN_CALLS = 0.15  # seconds between LLM calls to be polite\n",
        "MAX_RETRIES = 3            # retries on API failure\n",
        "MODEL_NAME = \"gemini-2.5-flash-lite\"  # default Gemini model (change if needed)\n",
        "\n",
        "# ---------------------------\n",
        "# Helper: secure API input for Gemini\n",
        "# ---------------------------\n",
        "def configure_gemini():\n",
        "    \"\"\"\n",
        "    Configures google.generativeai using environment variable GEMINI_API_KEY\n",
        "    or by securely prompting the user in Colab.\n",
        "    \"\"\"\n",
        "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    if not api_key:\n",
        "        # Prompt securely if running interactively\n",
        "        try:\n",
        "            from getpass import getpass\n",
        "            api_key = getpass(\"Paste your Google Gemini API Key (input hidden): \")\n",
        "        except Exception:\n",
        "            api_key = input(\"Paste your Google Gemini API Key: \")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Gemini API key was not provided.\")\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(f\"Gemini configured for model: {MODEL_NAME}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Helper: model call abstraction\n",
        "# ---------------------------\n",
        "def call_model(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls Gemini using the modern GenerativeModel interface with forced JSON output.\n",
        "    This guarantees structured output when combined with strict prompts.\n",
        "    \"\"\"\n",
        "    if not USE_GEMINI:\n",
        "        # Safe offline fallback (for debugging only)\n",
        "        text = prompt.lower()\n",
        "        if \"best\" in text or \"love\" in text or \"amazing\" in text:\n",
        "            return json.dumps({\"predicted_stars\": 5, \"explanation\": \"Positive sentiment detected.\"})\n",
        "        if \"worst\" in text or \"horrible\" in text or \"disgusting\" in text or \"terrible\" in text:\n",
        "            return json.dumps({\"predicted_stars\": 1, \"explanation\": \"Strong negative sentiment detected.\"})\n",
        "        return json.dumps({\"predicted_stars\": 3, \"explanation\": \"Neutral sentiment detected.\"})\n",
        "\n",
        "    # ✅ Proper Gemini usage (this is what fixes your JSON problem)\n",
        "    model = genai.GenerativeModel(MODEL_NAME)\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config={\n",
        "                    \"temperature\": 0.2,\n",
        "                    \"response_mime_type\": \"application/json\"  # ✅ THIS LINE FORCES JSON!\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if hasattr(response, \"text\") and response.text:\n",
        "                return response.text.strip()\n",
        "\n",
        "            # Fallback if SDK returns structured candidates\n",
        "            if hasattr(response, \"candidates\") and response.candidates:\n",
        "                return response.candidates[0].content.parts[0].text.strip()\n",
        "\n",
        "            return str(response)\n",
        "\n",
        "        except Exception:\n",
        "            time.sleep(0.5 * (attempt + 1))\n",
        "\n",
        "    raise RuntimeError(\"LLM call failed after retries.\")\n",
        "\n",
        "# ---------------------------\n",
        "# JSON cleaning & parsing utilities\n",
        "# ---------------------------\n",
        "def extract_json_object(text: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Attempts to robustly extract a JSON object from the model's text output.\n",
        "    Returns a dict on success, raises ValueError on failure.\n",
        "    \"\"\"\n",
        "    # Remove common markdown fences\n",
        "    cleaned = re.sub(r\"```(?:json)?\", \"\", text, flags=re.IGNORECASE).strip()\n",
        "    # Attempt to locate first {...} block\n",
        "    match = re.search(r\"\\{.*\\}\", cleaned, flags=re.DOTALL)\n",
        "    if match:\n",
        "        candidate = match.group(0)\n",
        "    else:\n",
        "        candidate = cleaned\n",
        "\n",
        "    # Some models use single quotes; normalize to double quotes for json.loads\n",
        "    candidate = candidate.replace(\"'\", '\"')\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(candidate)\n",
        "        return parsed\n",
        "    except Exception:\n",
        "        # Try to salvage minimal predicted_stars using regex\n",
        "        m = re.search(r'[\"\"]?predicted_stars[\"\"]?\\s*[:=]\\s*(\\d)', candidate)\n",
        "        if m:\n",
        "            return {\"predicted_stars\": int(m.group(1)), \"explanation\": \"Recovered via regex due to malformed JSON.\"}\n",
        "        raise ValueError(\"Unable to parse JSON from model output. Raw output truncated: \" + cleaned[:200])\n",
        "\n",
        "# ---------------------------\n",
        "# Prompt definitions (3 approaches) with STRICT JSON enforcement\n",
        "# ---------------------------\n",
        "\n",
        "# ✅ Approach A: Direct (Production-safe JSON)\n",
        "PROMPT_DIRECT = \"\"\"\n",
        "You are a strict JSON-only API.\n",
        "\n",
        "Task:\n",
        "Given a Yelp review, predict the star rating from 1 to 5.\n",
        "\n",
        "Rules:\n",
        "- Output ONLY valid JSON.\n",
        "- Do NOT add markdown.\n",
        "- Do NOT add comments.\n",
        "- Do NOT add extra text.\n",
        "- The output MUST match this schema exactly:\n",
        "\n",
        "{{\n",
        "  \"predicted_stars\": 1-5 integer,\n",
        "  \"explanation\": \"one short sentence\"\n",
        "}}\n",
        "\n",
        "Review:\n",
        "\"{review}\"\n",
        "\n",
        "Return JSON only.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_DIRECT_EXPL = \"Direct instruction with hard JSON schema enforcement to eliminate formatting errors.\"\n",
        "\n",
        "# ✅ Approach B: Few-Shot (Production-safe JSON)\n",
        "PROMPT_FEWSHOT = \"\"\"\n",
        "You are a strict JSON-only classification API.\n",
        "\n",
        "Here are examples:\n",
        "\n",
        "Input: \"Worst food ever, cold and tasteless.\"\n",
        "Output:\n",
        "{{\"predicted_stars\": 1, \"explanation\": \"Extremely negative dining experience.\"}}\n",
        "\n",
        "Input: \"It was okay, edible but boring.\"\n",
        "Output:\n",
        "{{\"predicted_stars\": 3, \"explanation\": \"Average experience with no strong positives or negatives.\"}}\n",
        "\n",
        "Input: \"Absolutely amazing! Best night ever.\"\n",
        "Output:\n",
        "{{\"predicted_stars\": 5, \"explanation\": \"Outstanding experience with very positive sentiment.\"}}\n",
        "\n",
        "Now classify the review below.\n",
        "\n",
        "Rules:\n",
        "- Output ONLY valid JSON.\n",
        "- Follow the exact key names.\n",
        "- No markdown.\n",
        "- No extra text.\n",
        "\n",
        "Review:\n",
        "\"{review}\"\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_FEWSHOT_EXPL = \"Few-shot prompt with explicit JSON-only constraint and structured examples.\"\n",
        "\n",
        "# ✅ Approach C: Chain-of-Thought (JSON-safe reasoning)\n",
        "PROMPT_COT = \"\"\"\n",
        "You are a strict JSON-only reasoning API.\n",
        "\n",
        "Task:\n",
        "Analyze the review internally and return:\n",
        "\n",
        "Rules:\n",
        "- Output ONLY valid JSON.\n",
        "- Keys MUST be exactly:\n",
        "  - \"reasoning\"\n",
        "  - \"predicted_stars\"\n",
        "- No markdown.\n",
        "- No extra text.\n",
        "\n",
        "Schema:\n",
        "{{\n",
        "  \"reasoning\": \"brief step-by-step sentiment reasoning\",\n",
        "  \"predicted_stars\": 1-5 integer\n",
        "}}\n",
        "\n",
        "Review:\n",
        "\"{review}\"\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_COT_EXPL = \"Chain-of-thought with forced JSON schema to preserve reasoning and machine readability.\"\n",
        "\n",
        "PROMPT_VERSIONS = [\n",
        "    (\"Direct\", PROMPT_DIRECT, PROMPT_DIRECT_EXPL),\n",
        "    (\"FewShot\", PROMPT_FEWSHOT, PROMPT_FEWSHOT_EXPL),\n",
        "    (\"ChainOfThought\", PROMPT_COT, PROMPT_COT_EXPL),\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset: Try Kaggle API download first; otherwise ask user to upload CSV\n",
        "# Dataset: omkarsabnis/yelp-reviews-dataset (Kaggle)\n",
        "# ---------------------------\n",
        "def load_kaggle_dataset(sample_size: int = SAMPLE_SIZE) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Attempts to download the Kaggle dataset using environment credentials.\n",
        "    If not possible, asks the user to upload a CSV file named 'yelp.csv'\n",
        "    or to mount with other means.\n",
        "    \"\"\"\n",
        "    kaggle_dataset_ref = \"omkarsabnis/yelp-reviews-dataset\"  # dataset identifier\n",
        "    # First, check if the user already put yelp.csv in the working directory\n",
        "    local_csvs = [p for p in os.listdir(\".\") if p.lower().endswith(\".csv\")]\n",
        "    if \"yelp.csv\" in local_csvs:\n",
        "        print(\"Found local 'yelp.csv' -- using that.\")\n",
        "        df = pd.read_csv(\"yelp.csv\").dropna(subset=[\"text\", \"stars\"])\n",
        "        return df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Try Kaggle CLI (requires kaggle.json or env vars)\n",
        "    try:\n",
        "        # If kaggle CLI not available, install\n",
        "        !kaggle --version\n",
        "    except Exception:\n",
        "        !pip install -q kaggle\n",
        "\n",
        "    # Check for kaggle credentials\n",
        "    kaggle_token_exists = os.path.exists(os.path.expanduser(\"~/.kaggle/kaggle.json\")) or \\\n",
        "                          (\"KAGGLE_USERNAME\" in os.environ and \"KAGGLE_KEY\" in os.environ)\n",
        "\n",
        "    if kaggle_token_exists:\n",
        "        # Attempt to download dataset\n",
        "        try:\n",
        "            # Create a temporary directory for dataset\n",
        "            tmpdir = tempfile.mkdtemp()\n",
        "            print(\"Attempting to download dataset from Kaggle into:\", tmpdir)\n",
        "            !kaggle datasets download -d {kaggle_dataset_ref} -p {tmpdir} --unzip -q\n",
        "            # Find a CSV inside tmpdir\n",
        "            csv_files = []\n",
        "            for root, _, files in os.walk(tmpdir):\n",
        "                for f in files:\n",
        "                    if f.lower().endswith(\".csv\"):\n",
        "                        csv_files.append(os.path.join(root, f))\n",
        "            if not csv_files:\n",
        "                raise FileNotFoundError(\"No CSV found in downloaded Kaggle dataset.\")\n",
        "            # Use the first CSV that contains 'review' or 'stars' columns\n",
        "            chosen = None\n",
        "            for c in csv_files:\n",
        "                try:\n",
        "                    peek = pd.read_csv(c, nrows=5)\n",
        "                    if {'text', 'stars'}.issubset(peek.columns):\n",
        "                        chosen = c\n",
        "                        break\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if chosen is None:\n",
        "                chosen = csv_files[0]\n",
        "            df = pd.read_csv(chosen).dropna(subset=[\"text\", \"stars\"])\n",
        "            print(f\"Loaded dataset CSV: {chosen}\")\n",
        "            return df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        except Exception as e:\n",
        "            print(\"Kaggle download failed:\", str(e))\n",
        "\n",
        "    # If we reach here, fallback to asking user to upload a file manually (Colab)\n",
        "    print(\"Please upload 'yelp.csv' (the Kaggle CSV) into the Colab session or place it in working dir.\")\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        # pick first CSV\n",
        "        for name in uploaded:\n",
        "            if name.lower().endswith(\".csv\"):\n",
        "                df = pd.read_csv(name).dropna(subset=[\"text\", \"stars\"])\n",
        "                return df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # As last fallback, raise informative error\n",
        "    raise FileNotFoundError(\n",
        "        \"Dataset not found. Please either:\\n\"\n",
        "        \"1) Place 'yelp.csv' in the working directory (must have 'text' and 'stars' columns), OR\\n\"\n",
        "        \"2) Provide Kaggle credentials (kaggle.json) and allow the script to download omkarsabnis/yelp-reviews-dataset, OR\\n\"\n",
        "        \"3) Upload a CSV via Colab file upload.\"\n",
        "    )\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation utilities\n",
        "# ---------------------------\n",
        "def evaluate_predictions(y_true: List[int], y_pred: List[int]) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Compute Accuracy and MAE.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    mae = float(np.mean(np.abs(np.array(y_true) - np.array(y_pred))))\n",
        "    return acc, mae\n",
        "\n",
        "def compute_json_validity_rate(parsed_results: List[Tuple[bool, dict]]) -> float:\n",
        "    \"\"\"\n",
        "    parsed_results: list of (is_valid_json, parsed_dict_or_none)\n",
        "    \"\"\"\n",
        "    valid_count = sum(1 for ok, _ in parsed_results if ok)\n",
        "    return valid_count / max(1, len(parsed_results))\n",
        "\n",
        "def reliability_score(preds_per_run: List[List[int]]) -> float:\n",
        "    \"\"\"\n",
        "    preds_per_run: list (runs) of lists (predictions for each sample)\n",
        "    Compute fraction of samples for which all runs agreed (exact agreement).\n",
        "    \"\"\"\n",
        "    runs = len(preds_per_run)\n",
        "    n = len(preds_per_run[0])\n",
        "    agree_count = 0\n",
        "    for idx in range(n):\n",
        "        values = [preds_per_run[r][idx] for r in range(runs)]\n",
        "        if len(set(values)) == 1:\n",
        "            agree_count += 1\n",
        "    return agree_count / n\n",
        "\n",
        "def pairwise_agreement(preds_per_run: List[List[int]]) -> float:\n",
        "    \"\"\"\n",
        "    Computes average pairwise agreement (Dice) between runs.\n",
        "    \"\"\"\n",
        "    runs = len(preds_per_run)\n",
        "    n = len(preds_per_run[0])\n",
        "    total_agree = 0\n",
        "    count_pairs = 0\n",
        "    for i in range(runs):\n",
        "        for j in range(i + 1, runs):\n",
        "            agree = sum(1 for k in range(n) if preds_per_run[i][k] == preds_per_run[j][k])\n",
        "            total_agree += agree / n\n",
        "            count_pairs += 1\n",
        "    return total_agree / count_pairs if count_pairs else 1.0\n",
        "\n",
        "# ---------------------------\n",
        "# Main experiment orchestration\n",
        "# ---------------------------\n",
        "def run_experiments(sample_size: int = SAMPLE_SIZE):\n",
        "    # 1) Load dataset\n",
        "    df = load_kaggle_dataset(sample_size)\n",
        "    # Ensure text and stars exist\n",
        "    df = df.dropna(subset=[\"text\", \"stars\"]).reset_index(drop=True)\n",
        "    # Limit to requested sample size (stratified if possible)\n",
        "    if len(df) > sample_size:\n",
        "        try:\n",
        "            # stratified sample by stars\n",
        "            df = df.groupby(\"stars\", group_keys=False).apply(\n",
        "                lambda x: x.sample(min(len(x), max(1, sample_size // df['stars'].nunique())))\n",
        "            ).reset_index(drop=True)\n",
        "            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "        except Exception:\n",
        "            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "    else:\n",
        "        df = df.copy()\n",
        "\n",
        "    texts = df[\"text\"].astype(str).tolist()\n",
        "    true_stars = df[\"stars\"].astype(int).tolist()\n",
        "\n",
        "    # 2) Prepare storage for metrics\n",
        "    comparison_rows = []\n",
        "\n",
        "    # 3) For each prompt version\n",
        "    for name, template, explanation in PROMPT_VERSIONS:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Running experiment: {name}\")\n",
        "        print(\"Prompt intention:\", explanation)\n",
        "        # show the prompt skeleton once\n",
        "        print(\"Prompt skeleton (example):\")\n",
        "        print(template.format(review=texts[0])[:800])\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Run the prompt RUNS_PER_PROMPT independent times\n",
        "        all_parsed_ok = []  # list of lists for each run: [(is_valid, parsed), ...]\n",
        "        all_predictions = []  # list of lists for each run: [preds...]\n",
        "\n",
        "        for run_idx in range(RUNS_PER_PROMPT):\n",
        "            print(f\"Run {run_idx+1}/{RUNS_PER_PROMPT} for {name}\")\n",
        "            run_parsed = []\n",
        "            run_preds = []\n",
        "            for i, review in enumerate(tqdm(texts, desc=f\"{name}-run{run_idx+1}\", leave=False)):\n",
        "                prompt_text = template.format(review=review)\n",
        "\n",
        "                # call model and parse robustly with retry\n",
        "                raw_output = None\n",
        "                parsed = None\n",
        "                ok = False\n",
        "                for attempt in range(MAX_RETRIES):\n",
        "                    try:\n",
        "                        raw_output = call_model(prompt_text)\n",
        "                        parsed = extract_json_object(raw_output)\n",
        "                        ok = True\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        # If parsing failed, try one more time (model output variability)\n",
        "                        time.sleep(0.5 + attempt * 0.5)\n",
        "                        continue\n",
        "\n",
        "                if not ok:\n",
        "                    # mark as invalid and default to neutral 3\n",
        "                    run_parsed.append((False, None))\n",
        "                    run_preds.append(3)\n",
        "                else:\n",
        "                    run_parsed.append((True, parsed))\n",
        "                    # Expect integer predicted_stars key\n",
        "                    if isinstance(parsed, dict) and (\"predicted_stars\" in parsed):\n",
        "                        try:\n",
        "                            val = int(parsed[\"predicted_stars\"])\n",
        "                            val = max(1, min(5, val))\n",
        "                        except Exception:\n",
        "                            val = 3\n",
        "                    elif isinstance(parsed, dict) and (\"stars\" in parsed):\n",
        "                        try:\n",
        "                            val = int(parsed[\"stars\"])\n",
        "                            val = max(1, min(5, val))\n",
        "                        except Exception:\n",
        "                            val = 3\n",
        "                    else:\n",
        "                        # Some CoT prompt uses \"predicted_stars\" but might be nested; fallback to regex\n",
        "                        m = re.search(r'(\\d)\\s*(?:stars|star)\\b', raw_output.lower())\n",
        "                        val = int(m.group(1)) if m else 3\n",
        "                    run_preds.append(val)\n",
        "\n",
        "                time.sleep(DELAY_BETWEEN_CALLS)\n",
        "\n",
        "            all_parsed_ok.append(run_parsed)\n",
        "            all_predictions.append(run_preds)\n",
        "\n",
        "        # 4) Aggregate metrics\n",
        "        # For JSON validity rate, count valid across the first run (or average across runs)\n",
        "        json_rates = [compute_json_validity_rate(run) for run in all_parsed_ok]\n",
        "        avg_json_validity = sum(json_rates) / len(json_rates)\n",
        "\n",
        "        # For accuracy and MAE, compute using the mode of predictions across runs (consensus)\n",
        "        # First compute consensus prediction per sample (mode of runs)\n",
        "        n_samples = len(texts)\n",
        "        consensus_preds = []\n",
        "        per_sample_std = []\n",
        "        for i in range(n_samples):\n",
        "            values = [all_predictions[r][i] for r in range(RUNS_PER_PROMPT)]\n",
        "            consensus = Counter(values).most_common(1)[0][0]\n",
        "            consensus_preds.append(consensus)\n",
        "            per_sample_std.append(statistics.pstdev(values))\n",
        "\n",
        "        acc, mae = evaluate_predictions(true_stars, consensus_preds)\n",
        "\n",
        "        # Reliability measures\n",
        "        exact_agreement_fraction = reliability_score(all_predictions)\n",
        "        avg_pairwise_agree = pairwise_agreement(all_predictions)\n",
        "        avg_std = sum(per_sample_std) / len(per_sample_std)\n",
        "\n",
        "        # Save run-level errors (count invalid JSONs as errors)\n",
        "        total_invalid = sum([sum(1 for ok, _ in run if not ok) for run in all_parsed_ok])\n",
        "        total_calls = RUNS_PER_PROMPT * n_samples\n",
        "\n",
        "        comparison_rows.append({\n",
        "            \"Approach\": name,\n",
        "            \"Accuracy\": acc,\n",
        "            \"MAE\": mae,\n",
        "            \"JSON_Validity_Rate\": avg_json_validity,\n",
        "            \"Exact_Agreement\": exact_agreement_fraction,\n",
        "            \"Pairwise_Agreement\": avg_pairwise_agree,\n",
        "            \"Avg_Pred_StdDev\": avg_std,\n",
        "            \"Invalid_JSONs\": total_invalid,\n",
        "            \"Total_Calls\": total_calls\n",
        "        })\n",
        "\n",
        "        # Optional: Save detailed outputs to CSV for inspection\n",
        "        out_df = pd.DataFrame({\n",
        "            \"text\": texts,\n",
        "            \"true_stars\": true_stars,\n",
        "            \"consensus_pred\": consensus_preds,\n",
        "            \"pred_stddev\": per_sample_std\n",
        "        })\n",
        "        out_filename = f\"predictions_{name}.csv\"\n",
        "        out_df.to_csv(out_filename, index=False)\n",
        "        print(f\"Saved detailed predictions to {out_filename}\")\n",
        "\n",
        "    # 5) Summary table\n",
        "    summary_df = pd.DataFrame(comparison_rows)\n",
        "    # nicer formatting for display\n",
        "    display_df = summary_df.copy()\n",
        "    display_df[\"Accuracy\"] = display_df[\"Accuracy\"].map(lambda x: f\"{x:.2%}\")\n",
        "    display_df[\"MAE\"] = display_df[\"MAE\"].map(lambda x: f\"{x:.3f}\")\n",
        "    display_df[\"JSON_Validity_Rate\"] = display_df[\"JSON_Validity_Rate\"].map(lambda x: f\"{x:.2%}\")\n",
        "    display_df[\"Exact_Agreement\"] = display_df[\"Exact_Agreement\"].map(lambda x: f\"{x:.2%}\")\n",
        "    display_df[\"Pairwise_Agreement\"] = display_df[\"Pairwise_Agreement\"].map(lambda x: f\"{x:.2%}\")\n",
        "    display_df[\"Avg_Pred_StdDev\"] = display_df[\"Avg_Pred_StdDev\"].map(lambda x: f\"{x:.3f}\")\n",
        "\n",
        "    print(\"\\n\\nFINAL COMPARISON TABLE\")\n",
        "    print(display_df.to_string(index=False))\n",
        "\n",
        "    # 6) Short automated discussion (human-editable)\n",
        "    print(\"\\n\\nBRIEF DISCUSSION / INTERPRETATION\")\n",
        "    for row in comparison_rows:\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Approach: {row['Approach']}\")\n",
        "        print(f\"  Accuracy: {row['Accuracy']:.3%}, MAE: {row['MAE']:.3f}\")\n",
        "        print(f\"  JSON Validity (avg): {row['JSON_Validity_Rate']:.2%}\")\n",
        "        print(f\"  Exact agreement across {RUNS_PER_PROMPT} runs: {row['Exact_Agreement']:.2%}\")\n",
        "        print(f\"  Pairwise agreement (avg): {row['Pairwise_Agreement']:.2%}\")\n",
        "        print(\"  Notes:\")\n",
        "        if row['Approach'] == \"Direct\":\n",
        "            print(\"    - Baseline prompt. Fast and concise but may produce more JSON formatting errors if model attempts to add commentary.\")\n",
        "        elif row['Approach'] == \"FewShot\":\n",
        "            print(\"    - Anchors numeric scale via examples; often improves calibration and reduces borderline mistakes.\")\n",
        "        else:\n",
        "            print(\"    - Chain-of-Thought encourages explicit reasoning, which can improve correctness but may increase verbosity and parsing issues.\")\n",
        "\n",
        "    print(\"\\nConcluding remarks:\")\n",
        "    print(\" - Compare Accuracy and JSON validity: a higher accuracy with low JSON validity may be less usable (extra parsing required).\")\n",
        "    print(\" - Reliability (agreement across runs) indicates how deterministic the chosen prompt is under the model; higher is better for production.\")\n",
        "    print(\" - If JSON validity is low for a chosen prompt, consider enforcing stricter formatting constraints or post-processing heuristics.\")\n",
        "\n",
        "    # Return the summary table for programmatic use\n",
        "    return summary_df\n",
        "\n",
        "# ---------------------------\n",
        "# Run full pipeline (configure Gemini if needed)\n",
        "# ---------------------------\n",
        "if USE_GEMINI:\n",
        "    configure_gemini()\n",
        "\n",
        "summary = run_experiments(SAMPLE_SIZE)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary.to_csv(\"comparison_summary.csv\", index=False)\n",
        "print(\"\\nSaved summary to 'comparison_summary.csv'.\")\n",
        "\n",
        "# End of script\n"
      ]
    }
  ]
}